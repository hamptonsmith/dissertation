% ---------------------------------------------------------------------------
\chapter{Conclusion and Future Research}\label{ch:conclusion}
% ---------------------------------------------------------------------------
This research has demonstrated that a combination of better component specifications achieved with a flexible and expressive mathematical system and a domain-specific prover carefully tailored to the sorts of VCs that arise from well-engineered programs can significantly impact the verifiability of software.  While faster and more advanced provers are often cited as the answer the software verification problem, this research has demonstrated that an extremely modest prover can compete with more traditional verification systems simply by exploiting properties and patterns inherent to software verification.  In short, we have confirmed our hypothesis: that programmers write code they believe works, and that the necessary insights for proving resulting proof obligations should therefore be shallow.

Beyond providing evidence against the conception that reasoning formally about programs is difficult, this research has also developed a prototype verification system that exemplifies a hybrid design philosophy, taking many of the best attributes of purely mathematical reasoning systems and blending them with a practical, imperative, object-based programming language.  As a result, RESOLVE now represents the only verification language to combine an imperative, object-based language with features like higher-order logic, first-class dependent types, and an extensible mathematical universe based on a traditional foundational theory.  In the process, we have designed and implemented a novel compromise for type-checking a language with dependent types.

As much as we would care to close the book on the verification problem with this research, we acknowledge this represents only a small step toward the eventual goal of the true push-button verifying compiler.  And, while we are pleased to have addressed a number of questions, we have also raised many more:

Clearly, our ideas on how to qualify a likely ``good'' step were not supported by the data.  However, the consistency with which our fitness function fails nonetheless suggests that a good function is out there---after all, we have one that describes ``bad'' steps.  An open question is to determine if our function fails to take into account important variables, or if it merely combines the variables it has in the wrong proportions.  One interesting experiment might be to use a genetic algorithm or some other general optimization technique to attempt to find a suitable function.

While we genuinely believe that our static typing system is now the most flexible of any language, practical or pure, we are already seeing some of the cost of that flexibility.  Despite a straightforward algorithm, mixing the use of explicit type parameters, implicit type parameters, and type theorems often has counter-intuitive results, causing statements that are certainly mathematically sound to fail type-checking.  A subtler algorithm based on a dependency tree rather that strict left-to-right evaluation might be able to statically establish such statements.  Similarly, when mixing multiple types with complex relationships, RESOLVE is often unable to break ties between multiple seemingly-equally-applicable functions.  A mechanism to prioritize those by giving the type-checking system insight into the relationships between \emph{functions} in addition to types would resolve many of these issues.

The scalability of the prover is always a serious concern.  After all, both the antecedent development step and the consequent exploration step suffer from combinatorial explosion---the former of space, the latter of time.  While we are heartened to find that, after the application of our heuristics, very few steps are generally required in the consequent exploration phase, this does little to address the issue with the antecedent development stage.  To maintain scalability, further research will need to be done to qualify useful developments and useless transformations.  We hypothosize that ultimately a feedback loop between consequent exploration and antecedent development might be more appropriate than our current two-phase algorithm.  This would represent a kind of simulated annealing wherein the consequent exploration would represent a random walk, and intermittent antecedent development would function as a hill-climb.

Finally, and most importantly, more research is required on the human component of verification.  We are excited to have contributed to this with a more intuitive mathematical system with which---we hope---a mathematician will feel more at home than with the currently available formal mathematical languages.  But the interaction of a human programmer with a programming system is a complex and under-appreciated problem in which we often do not see the programmers of the language as true users.  But if verification is to succeed, we must overcome these issues to develop a system that strikes the proper balance between the formal rigor required for successful verification and the deep insights needed to support the programmer and bring that rigor within reach.
